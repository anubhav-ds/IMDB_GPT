{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba17afd-0b8d-48a7-8a29-365c9e3942ad",
   "metadata": {},
   "source": [
    "# IMDB GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2082dc0-16c9-43be-be77-48f4f6a9cba1",
   "metadata": {},
   "source": [
    "Goal of this project is to use hugging face and pytorch to create a decoder only Causal LM generator model similar to GPT framework using the IMDB review dataset. We used hugging face to prepare the dataset and pytorch/lightening for model training. We also applied hyperparameter tuning and linear learning rate scheduler to optimize the training. \n",
    "\n",
    "For evaluation we generated few responses with prompts to see how it is generating and also evaluated test perplexity and loss, which were 97.27 and 4.5775 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a53bb-ff88-45ae-a44f-e98c09eb8e75",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d3a1d9-50ca-4629-9529-6dd996ad0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # or \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9b6449-3b1c-45e2-a9e3-2a339b858872",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = load_dataset('imdb')\n",
    "\n",
    "split = raw_ds['train'].train_test_split(test_size = 0.08, seed = 10)\n",
    "\n",
    "ds = {\n",
    "    'train': split['train'],\n",
    "    'val': split['test'],\n",
    "    'test': raw_ds['test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b13b493-75eb-4d1c-aa2d-58bcb652e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(raw_ds['train']))['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c134a4a-433c-4df9-897f-08e435d86cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 50257 pad: 50256 eos: 50256\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained('gpt2')\n",
    "tok.pad_token = tok.eos_token\n",
    "print(\"vocab:\", tok.vocab_size, \"pad:\", tok.pad_token_id, \"eos:\", tok.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d7fe0c-3940-4a0d-9083-023afdb01484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch): \n",
    "    return tok(batch['text'], add_special_tokens=True, truncation= False)\n",
    "\n",
    "tokenized = {}\n",
    "for split_name, dset in ds.items(): \n",
    "    tokenized[split_name] = dset.map(\n",
    "        tokenize, \n",
    "        batched = True, \n",
    "        remove_columns = dset.column_names, \n",
    "        desc=f\"Tokenizing {split_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bcbc3b4-56d0-411d-abe6-832d6fe02ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 23000\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 2000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 25000\n",
       " })}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cce196-eed7-4d24-909f-c74b1f6c836d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 26904\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 2347\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 28582\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate\n",
    "    block_size = 256\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = len(concatenated[\"input_ids\"])\n",
    "    # Drop remainder to make full blocks\n",
    "    total_len = (total_len // block_size) * block_size\n",
    "    result = {}\n",
    "    for k, vals in concatenated.items():\n",
    "        vals = vals[:total_len]\n",
    "        result[k] = [vals[i:i+block_size] for i in range(0, total_len, block_size)]\n",
    "    # Labels = inputs for CLM (no masking)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = {}\n",
    "\n",
    "for split_name, dset in tokenized.items(): \n",
    "    lm_dataset[split_name] = dset.map(\n",
    "        group_texts, \n",
    "        batched = True\n",
    "    )\n",
    "\n",
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eecae0e-511e-46d6-8b2f-e8122c007e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(lm_dataset['train']))['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e277ae16-89f2-4a87-a9d9-41d0c1970496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(tokenized['train']))['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93e8d619-cea6-47d6-a3bb-20a694480785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([32, 256])\n",
      "input_ids\n",
      "tensor([[ 4520, 24407,    11,  ...,   284,   651,   616],\n",
      "        [   11,  1141,   262,  ...,  1256,   517,   287],\n",
      "        [ 1577,   262,  7110,  ...,  3173,   994, 29847],\n",
      "        ...,\n",
      "        [  286,  1402,    11,  ...,  3382,   284,  1494],\n",
      "        [ 5299,  3899,   327,  ...,   284,  1064,   503],\n",
      "        [ 1220,  6927,  1671,  ..., 29847,  1671,  1220]])\n",
      "attention_mask torch.Size([32, 256])\n",
      "attention_mask\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "labels torch.Size([32, 256])\n",
      "labels\n",
      "tensor([[ 4520, 24407,    11,  ...,   284,   651,   616],\n",
      "        [   11,  1141,   262,  ...,  1256,   517,   287],\n",
      "        [ 1577,   262,  7110,  ...,  3173,   994, 29847],\n",
      "        ...,\n",
      "        [  286,  1402,    11,  ...,  3382,   284,  1494],\n",
      "        [ 5299,  3899,   327,  ...,   284,  1064,   503],\n",
      "        [ 1220,  6927,  1671,  ..., 29847,  1671,  1220]])\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tok,\n",
    "    mlm = False\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    lm_dataset[\"train\"],\n",
    "    batch_size=32,         \n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=15,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    lm_dataset[\"val\"],\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=15,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    lm_dataset[\"test\"],\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=15,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape) \n",
    "    print(k) \n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f760a-15d8-411e-b8e7-0f4a5a2f557a",
   "metadata": {},
   "source": [
    "## Model Training and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e6ff2c-273e-4439-831d-e99833e16dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15a38397-3516-438f-805d-c24e10acf429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "GPTModel                                                          --\n",
       "├─Embedding: 1-1                                                  15,360,000\n",
       "├─PositionalEncoding: 1-2                                         --\n",
       "│    └─Embedding: 2-1                                             524,288\n",
       "│    └─Dropout: 2-2                                               --\n",
       "├─TransformerEncoder: 1-3                                         --\n",
       "│    └─ModuleList: 2-3                                            --\n",
       "│    │    └─TransformerEncoderLayer: 3-1                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-2                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-3                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-4                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-5                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-6                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-7                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-8                          3,152,384\n",
       "├─LayerNorm: 1-4                                                  1,024\n",
       "├─Linear: 1-5                                                     15,360,000\n",
       "==========================================================================================\n",
       "Total params: 56,464,384\n",
       "Trainable params: 56,464,384\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module): \n",
    "    #creating a learnable positional encoding \n",
    "    def __init__(self, max_length = 2048, dims = 128): \n",
    "        super().__init__()\n",
    "        self.max_length = max_length \n",
    "        self.dims = dims \n",
    "        self.encoding = nn.Embedding(num_embeddings= self.max_length, embedding_dim= self.dims) \n",
    "        self.drop = nn.Dropout(0.1) \n",
    "        nn.init.normal_(self.encoding.weight, mean = 0.0, std = 0.02) \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #inputs will be in shape of B, 256(max length of chunks), 128 (or any other dim size)\n",
    "        B, T, D = inputs.shape \n",
    "        position = torch.arange(T, device = inputs.device).unsqueeze(0).expand(B, T)\n",
    "        x = inputs + self.encoding(position)\n",
    "        x = self.drop(x) \n",
    "        return x\n",
    "\n",
    "def causal_mask(seq_length = 256, device = None): \n",
    "    mask = torch.triu(torch.ones(seq_length,seq_length, dtype = torch.bool, device = device), diagonal = 1)\n",
    "    return mask\n",
    "    \n",
    "class GPTModel(nn.Module): \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size = 30000, \n",
    "        n_head = 16, \n",
    "        dims = 512, \n",
    "        n_layer = 8, \n",
    "        max_len = 1024, \n",
    "        pad_id = 0, \n",
    "    ): \n",
    "        super().__init__() \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dims = dims \n",
    "        self.n_head = n_head \n",
    "        self.n_layer = n_layer \n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.tok_emb = nn.Embedding(num_embeddings = self.vocab_size, \n",
    "                                    embedding_dim = self.dims, \n",
    "                                    padding_idx = self.pad_id if self.pad_id is not None else -1)\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(max_length = self.max_len, dims = self.dims) \n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model= self.dims, \n",
    "            nhead= self.n_head, \n",
    "            dim_feedforward= self.dims*4, \n",
    "            dropout= 0.1, \n",
    "            activation= 'gelu',\n",
    "            batch_first= True, \n",
    "            norm_first= True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers= self.n_layer) \n",
    "        self.layer_norm = nn.LayerNorm(self.dims) \n",
    "\n",
    "        self.lm_head = nn.Linear(self.dims, vocab_size, bias=False)\n",
    "        nn.init.normal_(self.tok_emb.weight, mean=0.0, std=0.02)\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels): \n",
    "        x = self.tok_emb(input_ids) \n",
    "        x = self.pos_emb(x) \n",
    "\n",
    "        B, T, D = x.shape \n",
    "        mask = causal_mask(T, device = x.device) \n",
    "        key_pad = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        out = self.encoder(x, mask = mask, src_key_padding_mask = key_pad) \n",
    "        out = self.layer_norm(out) \n",
    "\n",
    "        logits = self.lm_head(out) \n",
    "\n",
    "        loss = None \n",
    "\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, self.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "summary(GPTModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9df1d1c-90ce-4903-b2b1-3628d145b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "class LitGPT(L.LightningModule):\n",
    "    def __init__(self, model, lr = 0.0001, weight_decay = 0.01, warmup = 0.05):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup = warmup\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits, loss = self.model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, loss = self.model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        total_steps = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = max(1, int(self.warmup*total_steps))\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= warmup_steps, num_training_steps= total_steps)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f76ad451-b86e-47a4-ad5a-9e0c4bf13b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4218b4bf-f870-4daa-a267-4bff23be81d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | GPTModel | 51.5 M | train\n",
      "-------------------------------------------\n",
      "51.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.5 M    Total params\n",
      "205.904   Total estimated model params size (MB)\n",
      "89        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6117573809f641139e80581d7581797c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfd554f1613421da61b98104681855e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947b5627f044478a93dff6c5db42660e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94fb57433ec4093b6fd31c6c277a2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced485cb9ee346baa9aafd4d01a7d05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16caf3d1bf2e4ae7a0cf54be614a60cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08447571246e4902be7b66237e8090db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7791eb0333541b4a39f8802c606db01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e040642202ad43b699b6eeff23f6c240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e9f5c917d04bf5b617e67434e70d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04fc20746db4adf865837c9afcd3139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eeab3a5ab94e9ba49da23531a7f0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "model = GPTModel(\n",
    "    vocab_size= tok.vocab_size, \n",
    "    dims= 512, \n",
    "    n_head = 16, \n",
    "    n_layer = 8, \n",
    "    max_len = 1024,\n",
    "    pad_id = tok.pad_token_id\n",
    ").to(device)\n",
    "    \n",
    "checkpoint_cb = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "lit_model = LitGPT(model, lr = 0.0001)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs = 10,\n",
    "    accelerator = \"auto\",\n",
    "    precision=\"bf16-mixed\",  \n",
    "    log_every_n_steps = 20,\n",
    "    callbacks=[checkpoint_cb],\n",
    ")\n",
    "\n",
    "trainer.fit(lit_model, train_dataloaders = train_loader, val_dataloaders = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ee85701-12e7-4353-89a7-742ebaad67c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitGPT(\n",
       "  (model): GPTModel(\n",
       "    (tok_emb): Embedding(50257, 512, padding_idx=50256)\n",
       "    (pos_emb): PositionalEncoding(\n",
       "      (encoding): Embedding(1024, 512)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): Linear(in_features=512, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6af9a02b-dae0-4bb6-b06b-4ccc312365bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    "    eos_token_id: int | None = None,\n",
    "    device: str = \"cuda\",\n",
    "    use_autocast: bool = True,        # bf16/fp16 autocast on CUDA\n",
    "):\n",
    "    model.eval()\n",
    "    if eos_token_id is None:\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Encode prompt\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    input_ids = enc[\"input_ids\"].to(device)     # [1, T0]\n",
    "\n",
    "    # Respect model’s max_len (positional table)\n",
    "    max_len = model.pos_emb.encoding.num_embeddings\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # If too long, keep only the most recent window\n",
    "        if input_ids.size(1) > max_len:\n",
    "            input_ids = input_ids[:, -max_len:]\n",
    "\n",
    "        attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward\n",
    "        if use_autocast and torch.cuda.is_available():\n",
    "            dtype = torch.bfloat16\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "                logits, _ = model(input_ids, attention_mask, labels=None)\n",
    "        else:\n",
    "            logits, _ = model(input_ids, attention_mask, labels=None)\n",
    "\n",
    "        # Take last-step logits and apply temperature\n",
    "        next_logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "\n",
    "        # Top-k filtering\n",
    "        if top_k is not None and top_k > 0:\n",
    "            v, idx = torch.topk(next_logits, k=min(top_k, next_logits.size(-1)))\n",
    "            filtered = torch.full_like(next_logits, float(\"-inf\"))\n",
    "            next_logits = filtered.scatter(-1, idx, v)\n",
    "\n",
    "        # Top-p (nucleus) filtering\n",
    "        if top_p is not None and 0.0 < top_p < 1.0:\n",
    "            sorted_logits, sorted_idx = torch.sort(next_logits, descending=True)\n",
    "            probs = F.softmax(sorted_logits, dim=-1)\n",
    "            cumsum = torch.cumsum(probs, dim=-1)\n",
    "            cutoff = cumsum > top_p\n",
    "            # Keep at least one token\n",
    "            cutoff[..., 0] = False\n",
    "            sorted_logits[cutoff] = float(\"-inf\")\n",
    "            # Scatter back to original order\n",
    "            next_logits = torch.full_like(next_logits, float(\"-inf\")).scatter(-1, sorted_idx, sorted_logits)\n",
    "\n",
    "        # Sample or greedy\n",
    "        probs = F.softmax(next_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # for greedy: probs.argmax(-1, keepdim=True)\n",
    "\n",
    "        # Append\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        # Optional early stop on EOS\n",
    "        if eos_token_id is not None and next_token.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c18545f-41d2-4ffb-99a4-d5ecd3f4f54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was among the worst I have seen br /><br />Even it's a movie to be seen and it's a total waste of time. This is a bad movie. It has the best of the cast. The acting is good, this movie is really horrible. I've been to say, even if you're a kid.This is the worst movie i have ever seen. I have seen the title, but it was terrible.This was an over-top movie. I actually have seen it, but it was not so bad. A lot of the first part was great and the plot was bad.<br /><br\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This movie was among the worst I have seen \"\n",
    "\n",
    "model_to_gen = lit_model.model\n",
    "model_to_gen.to(device).eval()\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=120,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    device=device\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13836dd9-ac12-4c1a-be91-416cb3361522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie was great! *** <br /><br />The film is about the first three-one who, the film, is not the most part in the movie in which they are treated to the rest of the actors, but by the other, he's not the worst. The story line, from the opening, the camera, I thought it was the plot, and all that was pretty predictable. The whole movie is very long and the dialog just looks very well in the movie, which does not exist. It\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Movie was great! \"\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.90,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    device=device\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2faa623a-d418-40bc-a76d-50329d35cd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will train a bit more to see if performance can be improved\n",
    "\n",
    "best_model = checkpoint_cb.best_model_path\n",
    "\n",
    "model2 = GPTModel(\n",
    "    vocab_size=tok.vocab_size,\n",
    "    dims=512,\n",
    "    n_head=16,\n",
    "    n_layer=8,\n",
    "    max_len=1024,\n",
    "    pad_id=tok.pad_token_id\n",
    ")\n",
    "\n",
    "lit_model2 = LitGPT(model2, lr=5e-5, warmup= 0.02)\n",
    "\n",
    "state = torch.load(best_model, map_location = 'cpu')\n",
    "lit_model2.load_state_dict(state['state_dict'], strict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0b62a02-58e0-4fe8-8ac5-006e8633b679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:751: Checkpoint directory /mnt/d/ibm_genai/lightning_logs/version_23/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | GPTModel | 51.5 M | train\n",
      "-------------------------------------------\n",
      "51.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.5 M    Total params\n",
      "205.904   Total estimated model params size (MB)\n",
      "89        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5130001e1f9c4fd38c2b67fa103561cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e6833403414728b41c6a77ad6efaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fd99f497054bdab191aabd9e0f7ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b2977eacc14abfa0d1043c44aa4491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcf10ec4d9b488da32f1fbad2000442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216221de6ca549179e8cb77b46ddd139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed675a0fd28466ab2235a9a6512523a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae602971e4e4bd3badb5f982a1616cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b7307d191e4b268e3420b1b53d6b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b0b655ee9e4f05a1cccbad677527f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a132e3bcaea846f9a3e6eadb2f4c8204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505511191d4b45bc98d689603dd9870b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer2 = L.Trainer(\n",
    "    max_epochs=10,       \n",
    "    accelerator=\"auto\",\n",
    "    precision=\"bf16-mixed\",\n",
    "    log_every_n_steps=20,\n",
    "    callbacks=[checkpoint_cb])\n",
    "\n",
    "trainer2.fit(lit_model2, train_dataloaders = train_loader, val_dataloaders = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c87e7a5f-0c1c-4680-b7ca-94d5b5ce1f18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | GPTModel | 51.5 M | train\n",
      "-------------------------------------------\n",
      "51.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.5 M    Total params\n",
      "205.904   Total estimated model params size (MB)\n",
      "89        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6e9e9860ef4257aa397c65f91e9ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a7f2694b3c4986947b571481152d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8056d2f93a4472489e5b823b708196b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad460f163cf4a4fb9d030bd615026f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea33eeb423449d888f2480f3375ae06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf6c405cc284a8b8ca90bb31c1f3713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c1379c1e2c435c886efb40f112a751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0681b52a25043dc800a6e074bddf12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeee1553aea42bc93e4a619805950c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728a2a28ddbf4b87a7ee1d2496d72d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f965c2481f4cca8fcf8e0a99920c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b321f194354431a271c39ad9df073a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "#training a bit more with early stopping now\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "best_path = checkpoint_cb.best_model_path\n",
    "\n",
    "model2 = GPTModel(\n",
    "    vocab_size=tok.vocab_size,\n",
    "    dims=512, n_head=16, n_layer=8, max_len=1024,\n",
    "    pad_id=tok.pad_token_id\n",
    ")\n",
    "lit2 = LitGPT(model2, lr=5e-5)  # lower LR\n",
    "\n",
    "state = torch.load(best_path, map_location=\"cpu\")\n",
    "lit2.load_state_dict(state[\"state_dict\"], strict=True)\n",
    "\n",
    "ckpt2 = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "\n",
    "trainer2 = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"bf16-mixed\",\n",
    "    callbacks=[ckpt2, es],\n",
    "    log_every_n_steps=20,\n",
    ")\n",
    "trainer2.fit(lit2, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c50f82-f53f-4f1b-a752-78df13ff5e50",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39d5c0c7-9ef5-4f60-9be8-2c44e01d6680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss tuned | 4.2200  train_ppl tuned | 68.03\n",
      "val_loss tuned | 4.5102  val_ppl tuned | 90.94\n",
      "test_loss | 4.8314  test_ppl | 125.38\n",
      "test_loss tuned | 4.5775  test_ppl tuned | 97.27\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_perplexity(model, dataloader, device=\"cuda\", use_bf16=True):\n",
    "    model.eval().to(device)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        autocast_enabled = use_bf16 and torch.cuda.is_available()\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=autocast_enabled):\n",
    "            _, loss = model(input_ids, attention_mask, labels)\n",
    "\n",
    "        # count non-ignored targets after the 1-token shift\n",
    "        n_valid = (labels[:, 1:] != -100).sum().item()\n",
    "        total_loss += loss.item() * max(n_valid, 1)\n",
    "        total_tokens += max(n_valid, 1)\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(min(avg_loss, 10.0))\n",
    "    return avg_loss, ppl\n",
    "\n",
    "train_loss = 4.22\n",
    "train_ppl = math.exp(4.22)\n",
    "print(f\"train_loss tuned | {train_loss:.4f}  train_ppl tuned | {train_ppl:.2f}\")\n",
    "\n",
    "val_loss, val_ppl = eval_perplexity(model2, val_loader, device=device, use_bf16=True)\n",
    "print(f\"val_loss tuned | {val_loss:.4f}  val_ppl tuned | {val_ppl:.2f}\")\n",
    "\n",
    "test_loss, test_ppl = eval_perplexity(model, test_loader, device=device, use_bf16=True)\n",
    "print(f\"test_loss | {test_loss:.4f}  test_ppl | {test_ppl:.2f}\")\n",
    "\n",
    "test_loss, test_ppl = eval_perplexity(model2, test_loader, device=device, use_bf16=True)\n",
    "print(f\"test_loss tuned | {test_loss:.4f}  test_ppl tuned | {test_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c98d7d-55fb-49b5-991f-e8e60b576c6a",
   "metadata": {},
   "source": [
    "From results above we can see that while further hyperparameter tuning did help and we can improve test performance even more if we train few more epochs, difference between training and validation/test loss will increase signifying possible overfitting. Still, we improved perplexity by ~22% which is already a very good improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3172d3e5-1e02-4461-a13c-84e40fae40af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was among the worst I have seen. The story is simple and predictable. The character who is even stronger and less than a guy who makes an appearance in the movie, and the story is interesting and intelligent. I understand that I was able to feel this way through the movie. I was looking forward to seeing it in the movie, because it was almost like the other movie, but it just ended up being more believable. It wasn't a drama, just not a movie that would be funny. It did have some good parts. The movie was also bad. But that was the second half. The only reason I saw this movie\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This movie was among the worst I have seen\"\n",
    "\n",
    "model_to_gen = lit2.model\n",
    "model_to_gen.to(device).eval()\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=120,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c8ecec60-7de8-4f01-980d-8bb11b421acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was among the worst I have seen. I have never seen a horror movie. The acting was bad. I can't even understand the acting. The dialog was awful. I liked it all. I liked the characters, but the one, the plot was so bad that I was feeling cheated. My wife, the sister, was a guy. So the people were bad and I don't know what they were doing. The actors were good. I suppose they were the only reason they had. However, I was amazed that this was\n"
     ]
    }
   ],
   "source": [
    "#adjusting parameters a bit to get better sounding text\n",
    "\n",
    "prompt = \"This movie was among the worst I have seen\"\n",
    "\n",
    "model_to_gen = lit_model2.model\n",
    "model_to_gen.to(device).eval()\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.85,\n",
    "    top_k=60,\n",
    "    top_p=0.90,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "933eb26f-464c-46e3-9983-5110cff1e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie was Awesome! I can't wait for something for it. I've just been the most talented director of his other films. And while I watch this one, I think you're a fan of the first movie. I was not expecting much, because I haven't seen it. It's a very very well made movie, and I don't think that you can't really care if it's going to be a good film, but I'll give it a try.<br /><br />The acting is also\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Movie was Awesome!\"\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.85,\n",
    "    top_k=60,\n",
    "    top_p=0.90,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53dbad53-bc98-40dc-b999-3f3dc136d937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is great!I watched this movie a 5 year old and was watching it. It was a very different movie than I did it. I was waiting for the movie to get the impression of the characters. I thought that I thought it was a good movie. I am so disappointed that it was so very funny and\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This movie is great!\"\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.85,\n",
    "    top_k=60,\n",
    "    top_p=0.90,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7b0b775-6865-47b8-8bf2-74d574bf81b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was oot. I can't believe that the director had the opportunity to tell you why the film is all about. I mean, I would have liked to see the first two seasons before the first one, and they made it a long way into the series. But the writing was good, the ending was stupid and stupid, the acting was terrible. The problem was because it started with a good story and i couldn't. I still think this movie was directed by someone with a few good actors, but the way they turned it off was because of the real life in the movie was in it. The main character's character is so weird and the character is just totally different. The character really was terrible and the girl is cute. I think this movie isn't bad either, but not one of the worst movies ever made.<br /><br />If you're looking for a remake of the Dead in the Crypt, this movie's been made in a very early 90's movie. It's not a good movie because it's not an a movie. The plot is very good too, but sadly a mess of the first two movies are very touching. All the actors are very good. I saw this movie a lot better than that, but it was fun to watch. The plot is very good, but the actors are some of the dialogs are excellent, the acting, and the acting are great. There are so much better than the main characters who have the plot. The original plot is\n"
     ]
    }
   ],
   "source": [
    "#trying bigger prompt\n",
    "\n",
    "prompt = \"This movie was \"\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.85,\n",
    "    top_k=100,\n",
    "    top_p=0.95,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be899f6d-e8f2-4486-93fa-1a18bef7536a",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab24ea-ac2f-4718-9277-b5b2e99693a6",
   "metadata": {},
   "source": [
    "In this project we created a Causal LM from scratch using the IMDB dataset. We used Hugging and Pytorch to do it and we obtained 97.27  perplexity score compared to 90.94 for validation, meaning that overall model has learned generalized patterns very well as difference is not so major across them. \n",
    "\n",
    "We can improve the performance of the model by increasing the context size in batch from 256 to 512 and above, and we can also increase the layers and attention heads. However, even with his setup our VRAM on local machine was stretched to its limit so we need more hardware for that. We can also improve performance with more review data and even by training more epochs but improvement will be minor as we have reached near a plateau.\n",
    "\n",
    "Overall, we were able to create GPT like model using the IMBD reviews on our own local machine with good generative results and decent improvement in loss and perplexity after hyperparameter tuning. Of course, model is rambling a bit based on the eye test, but even Mistral and other quantized models ramble like this. Considering all that, a model with 50 million parameters gave us decent results, better than we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bd82e-bebe-4576-a397-cf87d08e9083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
