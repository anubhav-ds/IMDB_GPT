{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba17afd-0b8d-48a7-8a29-365c9e3942ad",
   "metadata": {},
   "source": [
    "# IMDB GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2082dc0-16c9-43be-be77-48f4f6a9cba1",
   "metadata": {},
   "source": [
    "Goal of this project is to use hugging face and pytorch to create a decoder only Causal LM generator model similar to GPT framework using the IMDB review dataset. We used hugging face to prepare the dataset and pytorch/lightening for model training. We also applied hyperparameter tuning and linear learning rate scheduler to optimize the training. \n",
    "\n",
    "For evaluation we generated few responses with prompts to see how it is generating and also evaluated test perplexity and loss, which were 97.27 and 4.5775 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a53bb-ff88-45ae-a44f-e98c09eb8e75",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d3a1d9-50ca-4629-9529-6dd996ad0c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 16:23:37.278601: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-19 16:23:37.336707: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-19 16:23:38.507748: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # or \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9b6449-3b1c-45e2-a9e3-2a339b858872",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = load_dataset('imdb')\n",
    "\n",
    "split = raw_ds['train'].train_test_split(test_size = 0.08, seed = 10)\n",
    "\n",
    "ds = {\n",
    "    'train': split['train'],\n",
    "    'val': split['test'],\n",
    "    'test': raw_ds['test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b13b493-75eb-4d1c-aa2d-58bcb652e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(raw_ds['train']))['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c134a4a-433c-4df9-897f-08e435d86cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 50257 pad: 50256 eos: 50256\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained('gpt2')\n",
    "tok.pad_token = tok.eos_token\n",
    "print(\"vocab:\", tok.vocab_size, \"pad:\", tok.pad_token_id, \"eos:\", tok.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d7fe0c-3940-4a0d-9083-023afdb01484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch): \n",
    "    return tok(batch['text'], add_special_tokens=True, truncation= False)\n",
    "\n",
    "tokenized = {}\n",
    "for split_name, dset in ds.items(): \n",
    "    tokenized[split_name] = dset.map(\n",
    "        tokenize, \n",
    "        batched = True, \n",
    "        remove_columns = dset.column_names, \n",
    "        desc=f\"Tokenizing {split_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bcbc3b4-56d0-411d-abe6-832d6fe02ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 23000\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 2000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 25000\n",
       " })}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cce196-eed7-4d24-909f-c74b1f6c836d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 26904\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 2347\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 28582\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate\n",
    "    block_size = 256\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = len(concatenated[\"input_ids\"])\n",
    "    # Drop remainder to make full blocks\n",
    "    total_len = (total_len // block_size) * block_size\n",
    "    result = {}\n",
    "    for k, vals in concatenated.items():\n",
    "        vals = vals[:total_len]\n",
    "        result[k] = [vals[i:i+block_size] for i in range(0, total_len, block_size)]\n",
    "    # Labels = inputs for CLM (no masking)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized = {}\n",
    "for split_name, dset in ds.items(): \n",
    "    tokenized[split_name] = dset.map(\n",
    "        tokenize, \n",
    "        batched = True, \n",
    "        remove_columns = dset.column_names, \n",
    "        desc=f\"Tokenizing {split_name}\"\n",
    "    )\n",
    "    \n",
    "lm_dataset = {}\n",
    "\n",
    "for split_name, dset in tokenized.items(): \n",
    "    lm_dataset[split_name] = dset.map(\n",
    "        group_texts, \n",
    "        batched = True\n",
    "    )\n",
    "\n",
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eecae0e-511e-46d6-8b2f-e8122c007e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(lm_dataset['train']))['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e277ae16-89f2-4a87-a9d9-41d0c1970496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(tokenized['train']))['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e8d619-cea6-47d6-a3bb-20a694480785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([32, 256])\n",
      "input_ids\n",
      "tensor([[  340,   373,   523,  ...,   286, 49555,   326],\n",
      "        [ 2497,   340,    13,  ...,   278,  1985, 37525],\n",
      "        [ 3354,   286,   262,  ...,   262,  2106,   286],\n",
      "        ...,\n",
      "        [  326,   257,  3665,  ...,   345,   836,   470],\n",
      "        [  510,   284,   617,  ...,  1220,  6927,  1671],\n",
      "        [ 3592,    13,   314,  ...,  1064,   683,  2712]])\n",
      "attention_mask torch.Size([32, 256])\n",
      "attention_mask\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "labels torch.Size([32, 256])\n",
      "labels\n",
      "tensor([[  340,   373,   523,  ...,   286, 49555,   326],\n",
      "        [ 2497,   340,    13,  ...,   278,  1985, 37525],\n",
      "        [ 3354,   286,   262,  ...,   262,  2106,   286],\n",
      "        ...,\n",
      "        [  326,   257,  3665,  ...,   345,   836,   470],\n",
      "        [  510,   284,   617,  ...,  1220,  6927,  1671],\n",
      "        [ 3592,    13,   314,  ...,  1064,   683,  2712]])\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tok,\n",
    "    mlm = False\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    lm_dataset[\"train\"],\n",
    "    batch_size=32,         \n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=15,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    lm_dataset[\"val\"],\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=15,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    lm_dataset[\"test\"],\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=15,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape) \n",
    "    print(k) \n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f760a-15d8-411e-b8e7-0f4a5a2f557a",
   "metadata": {},
   "source": [
    "## Model Training and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e6ff2c-273e-4439-831d-e99833e16dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15a38397-3516-438f-805d-c24e10acf429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "GPTModel                                                          --\n",
       "├─Embedding: 1-1                                                  15,360,000\n",
       "├─PositionalEncoding: 1-2                                         --\n",
       "│    └─Embedding: 2-1                                             524,288\n",
       "│    └─Dropout: 2-2                                               --\n",
       "├─TransformerEncoder: 1-3                                         --\n",
       "│    └─ModuleList: 2-3                                            --\n",
       "│    │    └─TransformerEncoderLayer: 3-1                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-2                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-3                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-4                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-5                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-6                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-7                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-8                          3,152,384\n",
       "├─LayerNorm: 1-4                                                  1,024\n",
       "├─Linear: 1-5                                                     15,360,000\n",
       "==========================================================================================\n",
       "Total params: 56,464,384\n",
       "Trainable params: 56,464,384\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module): \n",
    "    #creating a learnable positional encoding \n",
    "    def __init__(self, max_length = 2048, dims = 128): \n",
    "        super().__init__()\n",
    "        self.max_length = max_length \n",
    "        self.dims = dims \n",
    "        self.encoding = nn.Embedding(num_embeddings= self.max_length, embedding_dim= self.dims) \n",
    "        self.drop = nn.Dropout(0.1) \n",
    "        nn.init.normal_(self.encoding.weight, mean = 0.0, std = 0.02) \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #inputs will be in shape of B, 256(max length of chunks), 128 (or any other dim size)\n",
    "        B, T, D = inputs.shape \n",
    "        position = torch.arange(T, device = inputs.device).unsqueeze(0).expand(B, T)\n",
    "        x = inputs + self.encoding(position)\n",
    "        x = self.drop(x) \n",
    "        return x\n",
    "\n",
    "def causal_mask(seq_length = 256, device = None): \n",
    "    mask = torch.triu(torch.ones(seq_length,seq_length, dtype = torch.bool, device = device), diagonal = 1)\n",
    "    return mask\n",
    "    \n",
    "class GPTModel(nn.Module): \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size = 30000, \n",
    "        n_head = 16, \n",
    "        dims = 512, \n",
    "        n_layer = 8, \n",
    "        max_len = 1024, \n",
    "        pad_id = 0, \n",
    "    ): \n",
    "        super().__init__() \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dims = dims \n",
    "        self.n_head = n_head \n",
    "        self.n_layer = n_layer \n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.tok_emb = nn.Embedding(num_embeddings = self.vocab_size, \n",
    "                                    embedding_dim = self.dims, \n",
    "                                    padding_idx = self.pad_id if self.pad_id is not None else -1)\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(max_length = self.max_len, dims = self.dims) \n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model= self.dims, \n",
    "            nhead= self.n_head, \n",
    "            dim_feedforward= self.dims*4, \n",
    "            dropout= 0.1, \n",
    "            activation= 'gelu',\n",
    "            batch_first= True, \n",
    "            norm_first= True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers= self.n_layer) \n",
    "        self.layer_norm = nn.LayerNorm(self.dims) \n",
    "\n",
    "        self.lm_head = nn.Linear(self.dims, vocab_size, bias=False)\n",
    "        nn.init.normal_(self.tok_emb.weight, mean=0.0, std=0.02)\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels): \n",
    "        x = self.tok_emb(input_ids) \n",
    "        x = self.pos_emb(x) \n",
    "\n",
    "        B, T, D = x.shape \n",
    "        mask = causal_mask(T, device = x.device) \n",
    "        key_pad = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        out = self.encoder(x, mask = mask, src_key_padding_mask = key_pad) \n",
    "        out = self.layer_norm(out) \n",
    "\n",
    "        logits = self.lm_head(out) \n",
    "\n",
    "        loss = None \n",
    "\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, self.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "summary(GPTModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9df1d1c-90ce-4903-b2b1-3628d145b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "class LitGPT(L.LightningModule):\n",
    "    def __init__(self, model, lr = 0.0001, weight_decay = 0.01, warmup = 0.05):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup = warmup\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits, loss = self.model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, loss = self.model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        total_steps = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = max(1, int(self.warmup*total_steps))\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= warmup_steps, num_training_steps= total_steps)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f76ad451-b86e-47a4-ad5a-9e0c4bf13b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4218b4bf-f870-4daa-a267-4bff23be81d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | GPTModel | 51.5 M | train\n",
      "-------------------------------------------\n",
      "51.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.5 M    Total params\n",
      "205.904   Total estimated model params size (MB)\n",
      "89        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6117573809f641139e80581d7581797c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfd554f1613421da61b98104681855e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947b5627f044478a93dff6c5db42660e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94fb57433ec4093b6fd31c6c277a2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced485cb9ee346baa9aafd4d01a7d05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16caf3d1bf2e4ae7a0cf54be614a60cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08447571246e4902be7b66237e8090db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7791eb0333541b4a39f8802c606db01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e040642202ad43b699b6eeff23f6c240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e9f5c917d04bf5b617e67434e70d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04fc20746db4adf865837c9afcd3139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eeab3a5ab94e9ba49da23531a7f0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "model = GPTModel(\n",
    "    vocab_size= tok.vocab_size, \n",
    "    dims= 512, \n",
    "    n_head = 16, \n",
    "    n_layer = 8, \n",
    "    max_len = 1024,\n",
    "    pad_id = tok.pad_token_id\n",
    ").to(device)\n",
    "    \n",
    "checkpoint_cb = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "lit_model = LitGPT(model, lr = 0.0001)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs = 10,\n",
    "    accelerator = \"auto\",\n",
    "    precision=\"bf16-mixed\",  \n",
    "    log_every_n_steps = 20,\n",
    "    callbacks=[checkpoint_cb],\n",
    ")\n",
    "\n",
    "trainer.fit(lit_model, train_dataloaders = train_loader, val_dataloaders = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ee85701-12e7-4353-89a7-742ebaad67c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitGPT(\n",
       "  (model): GPTModel(\n",
       "    (tok_emb): Embedding(50257, 512, padding_idx=50256)\n",
       "    (pos_emb): PositionalEncoding(\n",
       "      (encoding): Embedding(1024, 512)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): Linear(in_features=512, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6af9a02b-dae0-4bb6-b06b-4ccc312365bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    "    eos_token_id: int | None = None,\n",
    "    device: str = \"cuda\",\n",
    "    use_autocast: bool = True,        # bf16/fp16 autocast on CUDA\n",
    "):\n",
    "    model.eval()\n",
    "    if eos_token_id is None:\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Encode prompt\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    input_ids = enc[\"input_ids\"].to(device)     # [1, T0]\n",
    "\n",
    "    # Respect model’s max_len (positional table)\n",
    "    max_len = model.pos_emb.encoding.num_embeddings\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # If too long, keep only the most recent window\n",
    "        if input_ids.size(1) > max_len:\n",
    "            input_ids = input_ids[:, -max_len:]\n",
    "\n",
    "        attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward\n",
    "        if use_autocast and torch.cuda.is_available():\n",
    "            dtype = torch.bfloat16\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "                logits, _ = model(input_ids, attention_mask, labels=None)\n",
    "        else:\n",
    "            logits, _ = model(input_ids, attention_mask, labels=None)\n",
    "\n",
    "        # Take last-step logits and apply temperature\n",
    "        next_logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "\n",
    "        # Top-k filtering\n",
    "        if top_k is not None and top_k > 0:\n",
    "            v, idx = torch.topk(next_logits, k=min(top_k, next_logits.size(-1)))\n",
    "            filtered = torch.full_like(next_logits, float(\"-inf\"))\n",
    "            next_logits = filtered.scatter(-1, idx, v)\n",
    "\n",
    "        # Top-p (nucleus) filtering\n",
    "        if top_p is not None and 0.0 < top_p < 1.0:\n",
    "            sorted_logits, sorted_idx = torch.sort(next_logits, descending=True)\n",
    "            probs = F.softmax(sorted_logits, dim=-1)\n",
    "            cumsum = torch.cumsum(probs, dim=-1)\n",
    "            cutoff = cumsum > top_p\n",
    "            # Keep at least one token\n",
    "            cutoff[..., 0] = False\n",
    "            sorted_logits[cutoff] = float(\"-inf\")\n",
    "            # Scatter back to original order\n",
    "            next_logits = torch.full_like(next_logits, float(\"-inf\")).scatter(-1, sorted_idx, sorted_logits)\n",
    "\n",
    "        # Sample or greedy\n",
    "        probs = F.softmax(next_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # for greedy: probs.argmax(-1, keepdim=True)\n",
    "\n",
    "        # Append\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        # Optional early stop on EOS\n",
    "        if eos_token_id is not None and next_token.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c18545f-41d2-4ffb-99a4-d5ecd3f4f54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was among the worst I have seen br /><br />Even it's a movie to be seen and it's a total waste of time. This is a bad movie. It has the best of the cast. The acting is good, this movie is really horrible. I've been to say, even if you're a kid.This is the worst movie i have ever seen. I have seen the title, but it was terrible.This was an over-top movie. I actually have seen it, but it was not so bad. A lot of the first part was great and the plot was bad.<br /><br\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This movie was among the worst I have seen \"\n",
    "\n",
    "model_to_gen = lit_model.model\n",
    "model_to_gen.to(device).eval()\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=120,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    device=device\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13836dd9-ac12-4c1a-be91-416cb3361522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie was great! *** <br /><br />The film is about the first three-one who, the film, is not the most part in the movie in which they are treated to the rest of the actors, but by the other, he's not the worst. The story line, from the opening, the camera, I thought it was the plot, and all that was pretty predictable. The whole movie is very long and the dialog just looks very well in the movie, which does not exist. It\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Movie was great! \"\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.90,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    device=device\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2faa623a-d418-40bc-a76d-50329d35cd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will train a bit more to see if performance can be improved\n",
    "\n",
    "best_model = checkpoint_cb.best_model_path\n",
    "\n",
    "model2 = GPTModel(\n",
    "    vocab_size=tok.vocab_size,\n",
    "    dims=512,\n",
    "    n_head=16,\n",
    "    n_layer=8,\n",
    "    max_len=1024,\n",
    "    pad_id=tok.pad_token_id\n",
    ")\n",
    "\n",
    "lit_model2 = LitGPT(model2, lr=5e-5, warmup= 0.02)\n",
    "\n",
    "state = torch.load(best_model, map_location = 'cpu')\n",
    "lit_model2.load_state_dict(state['state_dict'], strict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0b62a02-58e0-4fe8-8ac5-006e8633b679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:751: Checkpoint directory /mnt/d/ibm_genai/lightning_logs/version_23/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | GPTModel | 51.5 M | train\n",
      "-------------------------------------------\n",
      "51.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.5 M    Total params\n",
      "205.904   Total estimated model params size (MB)\n",
      "89        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5130001e1f9c4fd38c2b67fa103561cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e6833403414728b41c6a77ad6efaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fd99f497054bdab191aabd9e0f7ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b2977eacc14abfa0d1043c44aa4491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcf10ec4d9b488da32f1fbad2000442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216221de6ca549179e8cb77b46ddd139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed675a0fd28466ab2235a9a6512523a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae602971e4e4bd3badb5f982a1616cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b7307d191e4b268e3420b1b53d6b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b0b655ee9e4f05a1cccbad677527f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a132e3bcaea846f9a3e6eadb2f4c8204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505511191d4b45bc98d689603dd9870b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer2 = L.Trainer(\n",
    "    max_epochs=10,       \n",
    "    accelerator=\"auto\",\n",
    "    precision=\"bf16-mixed\",\n",
    "    log_every_n_steps=20,\n",
    "    callbacks=[checkpoint_cb])\n",
    "\n",
    "trainer2.fit(lit_model2, train_dataloaders = train_loader, val_dataloaders = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c87e7a5f-0c1c-4680-b7ca-94d5b5ce1f18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | GPTModel | 51.5 M | train\n",
      "-------------------------------------------\n",
      "51.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.5 M    Total params\n",
      "205.904   Total estimated model params size (MB)\n",
      "89        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6e9e9860ef4257aa397c65f91e9ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a7f2694b3c4986947b571481152d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8056d2f93a4472489e5b823b708196b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad460f163cf4a4fb9d030bd615026f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea33eeb423449d888f2480f3375ae06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf6c405cc284a8b8ca90bb31c1f3713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c1379c1e2c435c886efb40f112a751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0681b52a25043dc800a6e074bddf12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeee1553aea42bc93e4a619805950c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728a2a28ddbf4b87a7ee1d2496d72d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f965c2481f4cca8fcf8e0a99920c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b321f194354431a271c39ad9df073a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "#training a bit more with early stopping now\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "best_path = checkpoint_cb.best_model_path\n",
    "\n",
    "model2 = GPTModel(\n",
    "    vocab_size=tok.vocab_size,\n",
    "    dims=512, n_head=16, n_layer=8, max_len=1024,\n",
    "    pad_id=tok.pad_token_id\n",
    ")\n",
    "lit2 = LitGPT(model2, lr=5e-5)  # lower LR\n",
    "\n",
    "state = torch.load(best_path, map_location=\"cpu\")\n",
    "lit2.load_state_dict(state[\"state_dict\"], strict=True)\n",
    "\n",
    "ckpt2 = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "\n",
    "trainer2 = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"bf16-mixed\",\n",
    "    callbacks=[ckpt2, es],\n",
    "    log_every_n_steps=20,\n",
    ")\n",
    "trainer2.fit(lit2, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca30b5-f2dd-47be-85f2-fb6b4cff33b3",
   "metadata": {},
   "source": [
    "## QLoRA Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d4ace-37b4-448d-8ab2-9ea01be7a0cb",
   "metadata": {},
   "source": [
    "In this section we will do PEFT using QLoRA technique on TinyLlama 1.1B natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8088ad7-cd62-4c27-a6f1-26569b45260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d9f7522-3ba7-4de0-8429-6bf063c8a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(llama_model, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "pad_id = tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "996f4b23-39f4-4ea5-ad7c-6a87cd9cec03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since vocab size and tokenizer is different we need to create datasets again\n",
    "tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab6b5135-f648-4524-8bf6-0546bb9cb79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([32, 256])\n",
      "input_ids\n",
      "tensor([[ 2714,   393, 11131,  ...,   322,  4120,  1711],\n",
      "        [29966,  1182,  2900,  ...,  2678, 19423,  1182],\n",
      "        [ 1090,  5011,   315,  ..., 10201,   679,   304],\n",
      "        ...,\n",
      "        [  281,  3825,  1728,  ...,  8031,   304,  1074],\n",
      "        [  450,  6996,  6492,  ...,   450,  2471, 29892],\n",
      "        [29873, 29892,   366,  ...,  2592,   304,  1375]])\n",
      "attention_mask torch.Size([32, 256])\n",
      "attention_mask\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "labels torch.Size([32, 256])\n",
      "labels\n",
      "tensor([[ 2714,   393, 11131,  ...,   322,  4120,  1711],\n",
      "        [29966,  1182,  2900,  ...,  2678, 19423,  1182],\n",
      "        [ 1090,  5011,   315,  ..., 10201,   679,   304],\n",
      "        ...,\n",
      "        [  281,  3825,  1728,  ...,  8031,   304,  1074],\n",
      "        [  450,  6996,  6492,  ...,   450,  2471, 29892],\n",
      "        [29873, 29892,   366,  ...,  2592,   304,  1375]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = {}\n",
    "for split_name, dset in ds.items(): \n",
    "    tokenized[split_name] = dset.map(\n",
    "        tokenize, \n",
    "        batched = True, \n",
    "        remove_columns = dset.column_names, \n",
    "        desc=f\"Tokenizing {split_name}\"\n",
    "    )\n",
    "    \n",
    "lm_dataset = {}\n",
    "\n",
    "for split_name, dset in tokenized.items(): \n",
    "    lm_dataset[split_name] = dset.map(\n",
    "        group_texts, \n",
    "        batched = True\n",
    "    )\n",
    "\n",
    "lm_dataset\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tok,\n",
    "    mlm = False\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    lm_dataset[\"train\"],\n",
    "    batch_size=32,         \n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=15,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    lm_dataset[\"val\"],\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=15,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    lm_dataset[\"test\"],\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=15,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape) \n",
    "    print(k) \n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "636e0e0f-8078-4972-9073-9fa5e1472564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16, \n",
    "    bnb_4bit_quant_type = 'nf4', \n",
    "    bnb_4bit_use_double_quant = True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_model, \n",
    "    quantization_config = quant_config, \n",
    "    device_map = 'auto', \n",
    "    dtype = torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b4f5aa7-ea57-40a5-9720-34a57f836de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.pad_token_id\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b97b82b-bac1-43ec-8107-d73fd30d9fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "\n",
    "# we will use linear layers here as target layers for lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fe398a7-4eda-4802-a71d-908760f2701b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "LlamaForCausalLM                                   --\n",
       "├─LlamaModel: 1-1                                  --\n",
       "│    └─Embedding: 2-1                              (65,536,000)\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                 (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-2                 (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-3                 (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-4                 (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-5                 (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-6                 (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-7                 (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-8                 (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-9                 (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-10                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-11                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-12                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-13                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-14                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-15                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-16                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-17                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-18                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-19                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-20                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-21                (22,024,192)\n",
       "│    │    └─LlamaDecoderLayer: 3-22                (22,024,192)\n",
       "│    └─LlamaRMSNorm: 2-3                           (2,048)\n",
       "│    └─LlamaRotaryEmbedding: 2-4                   --\n",
       "├─Linear: 1-2                                      (65,536,000)\n",
       "===========================================================================\n",
       "Total params: 615,606,272\n",
       "Trainable params: 0\n",
       "Non-trainable params: 615,606,272\n",
       "==========================================================================="
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "467de365-858a-4b74-a189-33e5371927a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,307,840 || all params: 1,106,356,224 || trainable%: 0.5701\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type= TaskType.CAUSAL_LM, \n",
    "    r = 8, \n",
    "    lora_alpha= 16, \n",
    "    lora_dropout= 0.08, \n",
    "    target_modules= [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config) \n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0a01f3c-b19f-4010-8312-89ccecdda742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.08, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.08, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.08, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.08, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.08, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.08, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.08, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a37c03be-ec4a-4088-bdd4-4ef60fe6244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer \n",
    "\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "args = TrainingArguments(\n",
    "    eval_strategy= 'steps',\n",
    "    output_dir= 'lora_llama/', \n",
    "    per_device_eval_batch_size = 16, \n",
    "    per_device_train_batch_size= 16, \n",
    "    gradient_accumulation_steps= 16, \n",
    "    learning_rate= 0.0005, \n",
    "    num_train_epochs= 2, \n",
    "    logging_steps = 30, \n",
    "    weight_decay= 0.005, \n",
    "    bf16= True, \n",
    "    eval_steps = 40,\n",
    "    warmup_steps = 40, \n",
    "    save_steps= 100,\n",
    "    optim = \"adamw_torch\", \n",
    "    report_to= 'none',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = args, \n",
    "    data_collator= data_collator, \n",
    "    train_dataset= lm_dataset['train'], \n",
    "    eval_dataset= lm_dataset['val'], \n",
    "    processing_class = tok\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "440d0196-c2f5-4e5f-8c27-afcfcdd9691a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 3:14:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.898000</td>\n",
       "      <td>2.770617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.767000</td>\n",
       "      <td>2.744558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.732900</td>\n",
       "      <td>2.735170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.707500</td>\n",
       "      <td>2.731399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.699700</td>\n",
       "      <td>2.727692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/anubh/miniconda3/envs/dl-env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=238, training_loss=2.7423425882804295, metrics={'train_runtime': 11717.1081, 'train_samples_per_second': 5.187, 'train_steps_per_second': 0.02, 'total_flos': 9.71561881388974e+16, 'train_loss': 2.7423425882804295, 'epoch': 2.0})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bf1ddfa1-bda4-4a2d-9f7f-9df1fb38fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('lora_llama/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf6eceda-b96b-4685-b0b0-30bf2641810f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2183' max='166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [166/166 29:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "eval_loss = metrics[\"eval_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd3457fd-c36e-410f-a4a2-23074c121d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss=2.7264, eval_ppl=15.28\n"
     ]
    }
   ],
   "source": [
    "print(f\"eval_loss={eval_loss:.4f}, eval_ppl={math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba5daab1-c4ac-4cd3-858d-1d4b31af4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "tok.padding_side = \"left\" \n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "def generate(prompt, max_new_tokens=120, temperature=0.9, top_p=0.95):\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091f5fd-2053-4d85-9bc7-9ff81ae4cd18",
   "metadata": {},
   "source": [
    "Validation Loss has generally converged to 2.72 and while more training can help in it, difference between Training Loss and Validation Loss would become bigger too, signifying potential training plateau in upcoming steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c50f82-f53f-4f1b-a752-78df13ff5e50",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39d5c0c7-9ef5-4f60-9be8-2c44e01d6680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss tuned | 4.2200  train_ppl tuned | 68.03\n",
      "val_loss tuned | 4.5102  val_ppl tuned | 90.94\n",
      "test_loss | 4.8314  test_ppl | 125.38\n",
      "test_loss tuned | 4.5775  test_ppl tuned | 97.27\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_perplexity(model, dataloader, device=\"cuda\", use_bf16=True):\n",
    "    model.eval().to(device)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        autocast_enabled = use_bf16 and torch.cuda.is_available()\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=autocast_enabled):\n",
    "            _, loss = model(input_ids, attention_mask, labels)\n",
    "\n",
    "        # count non-ignored targets after the 1-token shift\n",
    "        n_valid = (labels[:, 1:] != -100).sum().item()\n",
    "        total_loss += loss.item() * max(n_valid, 1)\n",
    "        total_tokens += max(n_valid, 1)\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(min(avg_loss, 10.0))\n",
    "    return avg_loss, ppl\n",
    "\n",
    "train_loss = 4.22\n",
    "train_ppl = math.exp(4.22)\n",
    "print(f\"train_loss tuned | {train_loss:.4f}  train_ppl tuned | {train_ppl:.2f}\")\n",
    "\n",
    "val_loss, val_ppl = eval_perplexity(model2, val_loader, device=device, use_bf16=True)\n",
    "print(f\"val_loss tuned | {val_loss:.4f}  val_ppl tuned | {val_ppl:.2f}\")\n",
    "\n",
    "test_loss, test_ppl = eval_perplexity(model, test_loader, device=device, use_bf16=True)\n",
    "print(f\"test_loss | {test_loss:.4f}  test_ppl | {test_ppl:.2f}\")\n",
    "\n",
    "test_loss, test_ppl = eval_perplexity(model2, test_loader, device=device, use_bf16=True)\n",
    "print(f\"test_loss tuned | {test_loss:.4f}  test_ppl tuned | {test_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c98d7d-55fb-49b5-991f-e8e60b576c6a",
   "metadata": {},
   "source": [
    "From results above we can see that while further hyperparameter tuning did help and we can improve test performance even more if we train few more epochs, difference between training and validation/test loss will increase signifying possible overfitting. Still, we improved perplexity by ~22% which is already a very good improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3172d3e5-1e02-4461-a13c-84e40fae40af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was among the worst I have seen. The story is simple and predictable. The character who is even stronger and less than a guy who makes an appearance in the movie, and the story is interesting and intelligent. I understand that I was able to feel this way through the movie. I was looking forward to seeing it in the movie, because it was almost like the other movie, but it just ended up being more believable. It wasn't a drama, just not a movie that would be funny. It did have some good parts. The movie was also bad. But that was the second half. The only reason I saw this movie\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This movie was among the worst I have seen\"\n",
    "\n",
    "model_to_gen = lit2.model\n",
    "model_to_gen.to(device).eval()\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=120,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c8ecec60-7de8-4f01-980d-8bb11b421acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was among the worst I have seen. I have never seen a horror movie. The acting was bad. I can't even understand the acting. The dialog was awful. I liked it all. I liked the characters, but the one, the plot was so bad that I was feeling cheated. My wife, the sister, was a guy. So the people were bad and I don't know what they were doing. The actors were good. I suppose they were the only reason they had. However, I was amazed that this was\n"
     ]
    }
   ],
   "source": [
    "#adjusting parameters a bit to get better sounding text\n",
    "\n",
    "prompt = \"This movie was among the worst I have seen\"\n",
    "\n",
    "model_to_gen = lit_model2.model\n",
    "model_to_gen.to(device).eval()\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.85,\n",
    "    top_k=60,\n",
    "    top_p=0.90,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "933eb26f-464c-46e3-9983-5110cff1e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie was Awesome! I can't wait for something for it. I've just been the most talented director of his other films. And while I watch this one, I think you're a fan of the first movie. I was not expecting much, because I haven't seen it. It's a very very well made movie, and I don't think that you can't really care if it's going to be a good film, but I'll give it a try.<br /><br />The acting is also\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Movie was Awesome!\"\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.85,\n",
    "    top_k=60,\n",
    "    top_p=0.90,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53dbad53-bc98-40dc-b999-3f3dc136d937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is great!I watched this movie a 5 year old and was watching it. It was a very different movie than I did it. I was waiting for the movie to get the impression of the characters. I thought that I thought it was a good movie. I am so disappointed that it was so very funny and\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This movie is great!\"\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.85,\n",
    "    top_k=60,\n",
    "    top_p=0.90,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7b0b775-6865-47b8-8bf2-74d574bf81b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was oot. I can't believe that the director had the opportunity to tell you why the film is all about. I mean, I would have liked to see the first two seasons before the first one, and they made it a long way into the series. But the writing was good, the ending was stupid and stupid, the acting was terrible. The problem was because it started with a good story and i couldn't. I still think this movie was directed by someone with a few good actors, but the way they turned it off was because of the real life in the movie was in it. The main character's character is so weird and the character is just totally different. The character really was terrible and the girl is cute. I think this movie isn't bad either, but not one of the worst movies ever made.<br /><br />If you're looking for a remake of the Dead in the Crypt, this movie's been made in a very early 90's movie. It's not a good movie because it's not an a movie. The plot is very good too, but sadly a mess of the first two movies are very touching. All the actors are very good. I saw this movie a lot better than that, but it was fun to watch. The plot is very good, but the actors are some of the dialogs are excellent, the acting, and the acting are great. There are so much better than the main characters who have the plot. The original plot is\n"
     ]
    }
   ],
   "source": [
    "#trying bigger prompt\n",
    "\n",
    "prompt = \"This movie was \"\n",
    "\n",
    "out = generate(\n",
    "    model_to_gen, tok, prompt,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.85,\n",
    "    top_k=100,\n",
    "    top_p=0.95,\n",
    "    device=device\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fe65e1e2-95b6-46e7-a782-ecfae3564c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 2.7311458587646484, 'test_runtime': 1586.734, 'test_samples_per_second': 20.335, 'test_steps_per_second': 1.271, 'epoch': 2.0}\n",
      "Test loss: 2.7311 | Test PPL: 15.35\n"
     ]
    }
   ],
   "source": [
    "#qlora evaluation \n",
    "\n",
    "test_metrics = trainer.evaluate(eval_dataset = lm_dataset[\"test\"], metric_key_prefix=\"test\")\n",
    "print(test_metrics)  \n",
    "\n",
    "test_ppl = math.exp(test_metrics[\"test_loss\"])\n",
    "print(f\"Test loss: {test_metrics['test_loss']:.4f} | Test PPL: {test_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8a2cbc57-0417-4d01-b4b5-d24e8f034f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie is great! 8.7/10 This film was so much better than the others in the series. While the first movie was more of a horror film than a comedy (even though a comedy was probably intended), this one was very well done. It had great special effects, and the story was very well developed. All of the actors were excellent, and the jokes were very funny. My favorite line was: \"It\\'s a bunny rabbit, it\\'s a... \" (this line makes me laugh every time!). There were also a number of funny lines like'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('This movie is great! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "67c56758-c983-4175-b521-b3ca0d3c3073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Movie was Awesome! In my opinion, this is one of the funniest movies ever made. The cast is exceptional, the story is very believable, and the acting is superb. As a movie, it is very well acted, written, and directed. One of the many reasons I rated this movie \"10\" is because it is very well acted, with a great cast. <br /><br />The story is about a very unstable woman who is in a coma (the movie was made in 1971) and when she wakes up'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('Movie was Awesome!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8cce9607-c740-48f0-af70-21e4fca2c652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This movie was among the worst I have seen. I do not understand the negative reviews. It's not that I didn't like it...I didn't. It's that you can't make this story out in a movie. It just doesn't work. The actors are not up to the task and this is a total waste of time. 1/10 I love movies about the middle east and I have watched a lot of them. I will watch almost any of them provided the story is good, even if the movie is a stereotype of the Middle East, which is what\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"This movie was among the worst I have seen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba544b87-42ce-4a62-8e22-276c3328527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie was 100% great! <br /><br />It was the first movie I ever saw that had an American accent. I loved the accent, and even though I didn\\'t know what was going to happen, I was still able to follow the story. I was really surprised when I watched the end. I thought it would be a typical movie ending like I\\'ve seen in many other movies. But the ending was different. I was really surprised by it. There are many good things about this movie. Its beautiful color scheme. The cinematography is stunning. The music is beautiful. The photography is fantastic. It has a great story line. The movie itself is so beautiful and has so many great characters and scenes. The plot is very intriguing. I think that the movie is so great because the story is so captivating and the cinematography is so breathtaking. It is a great movie to watch in the evening. It is so beautiful that it is worth watching it more than once. I would definitely recommend this movie. It is a great movie that you should see and enjoy! I highly recommend this movie! It is a perfect film for people who love beautiful films with great acting and story lines. It is a very beautiful movie. I would give this movie a 10/10. This is not a typical \"King of Kung Fu\" film. The plot is not really \"'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('This movie was ', temperature= 0.85, max_new_tokens= 300, top_p= 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "70e0ba1f-ac86-4a09-a0a0-9928fdb3010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Director 13 Days in September (Nolte, 1988). This is a movie I have seen many times, but have always enjoyed it. There are scenes that I remember so vividly I've looked them up on the IMDb database, so there are times I replay scenes from memory, with the film always playing in the background. The film is set in an area of India, and is a story of a young boy, who falls in love with a girl who has to marry another man, and goes into hiding. Along the way the girl's\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('Director ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a9b4b9fb-f57f-456b-ba7c-1d5e10689d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A 3 out of 10 for this one.<br /><br />2. 'Mindhunter' A 3 out of 10 for this one.<br /><br />I was really looking forward to this movie. I had heard about the cast and crew and was looking forward to the story. The movie started promisingly but the end was predictable.<br /><br />If you love horror movies of the 1970's you'll love this one, but if you don't like the '70's then you\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('A ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c2056-e1a7-48ef-bb25-fa7f1e65e699",
   "metadata": {},
   "source": [
    "We cant compare loss and perpelexity across both models because even if we used similar process, tokenization of pytorch Causal LM was done using the gpt-2 tokenizer, so while we cant compare them strictly, we can still compare them to some level. Visual inspection tell us that our QLoRA Finetuning worked very well as even with prompt containing \"A\" it was able to create Imdb based movie reviews, and while there is still rambling present to some level, overall performance is excellent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be899f6d-e8f2-4486-93fa-1a18bef7536a",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab24ea-ac2f-4718-9277-b5b2e99693a6",
   "metadata": {},
   "source": [
    "In this project we created a Causal LM from scratch using the IMDB dataset. We used Hugging and Pytorch to do it and we obtained 97.27  perplexity score compared to 90.94 for validation, meaning that overall model has learned generalized patterns very well as difference is not so major across them. \n",
    "\n",
    "We can improve the performance of the model by increasing the context size in batch from 256 to 512 and above, and we can also increase the layers and attention heads. However, even with his setup our VRAM on local machine was stretched to its limit so we need more hardware for that. We can also improve performance with more review data and even by training more epochs but improvement will be minor as we have reached near a plateau.\n",
    "\n",
    "We then finetuned TinyLlama locally using QLoRA Quantization and were successful in finetuning the model as even very neutral prompt it was able to generate Movie review. We also got test perplexity of 15.35, which may not be strictly comparable to the pytorch model but pairing it with visual inspection and testing of prompts, its a clear indication that finetuned TinyLlama model worked very well with more human like text generation.\n",
    "\n",
    "Overall, we were able to create GPT like model using the IMBD reviews on our own local machine with good generative results and decent improvement in loss and perplexity after hyperparameter tuning. We then finetuned a TinyLlama model locally by using QLoRA finetuning based on the same dataset and pipeline and obtained much improved results in form of reduced perplexity and better sounding movie reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
